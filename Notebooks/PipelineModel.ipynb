{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PipelineModel.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1WEFaR76FfaYTfwWkuKC858jytirOqA48","authorship_tag":"ABX9TyOVIEwqR88F4oMABYua6oRK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Iwf1-ThKr9Gu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593612805434,"user_tz":-60,"elapsed":4744,"user":{"displayName":"nuno amaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoA9ByRphWWfFL33pNwbcmcsHIaScd_KC29jxGqA=s64","userId":"06393260799044096547"}}},"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import re\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn import metrics\n","from tqdm.autonotebook import tqdm"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFcZyd1oxzkw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593612805436,"user_tz":-60,"elapsed":4714,"user":{"displayName":"nuno amaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoA9ByRphWWfFL33pNwbcmcsHIaScd_KC29jxGqA=s64","userId":"06393260799044096547"}},"outputId":"36ddcbcb-ac07-453c-bb3f-018f5e908678"},"source":["t = torch.cuda.get_device_properties(0).total_memory\n","print('Total Memory: {}'.format(t / 1e9))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Total Memory: 15.812263936\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Aptsgc3HaUD1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593612815622,"user_tz":-60,"elapsed":14885,"user":{"displayName":"nuno amaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoA9ByRphWWfFL33pNwbcmcsHIaScd_KC29jxGqA=s64","userId":"06393260799044096547"}},"outputId":"cb86cfc1-a537-4827-c345-1497b19c678a"},"source":["!pip install simpletransformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting simpletransformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/c1/e34ae3bc246fb35b92c7f052705ffa88ec70bb998e9774a2dcff00465dad/simpletransformers-0.40.2-py3-none-any.whl (190kB)\n","\u001b[K     |████████████████████████████████| 194kB 9.3MB/s \n","\u001b[?25hCollecting tokenizers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/15/1c026f3aeafd26db30cb633d9915aae666a415179afa5943263e5dbd55a6/tokenizers-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 9.3MB/s \n","\u001b[?25hCollecting transformers>=2.11.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n","\u001b[K     |████████████████████████████████| 757kB 52.5MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.18.5)\n","Collecting seqeval\n","  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.0.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.22.2.post1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (4.41.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.23.0)\n","Collecting tensorboardx\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n","\u001b[K     |████████████████████████████████| 204kB 53.0MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->simpletransformers) (0.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->simpletransformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 57.1MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 51.8MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->simpletransformers) (20.4)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->simpletransformers) (2.3.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2.8.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->simpletransformers) (0.15.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2.9)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (3.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.11.0->simpletransformers) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.11.0->simpletransformers) (2.4.7)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.1.2)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (47.3.1)\n","Building wheels for collected packages: seqeval, sacremoses\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=31397abc0f4d40a55290aa1795acc070f0885425d2d191fc0af0b29aba057007\n","  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=e780535eb07bf4229e8290e00c84192cd4d8427cd2d630939a0da537ce13b0d5\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built seqeval sacremoses\n","\u001b[31mERROR: transformers 3.0.0 has requirement tokenizers==0.8.0-rc4, but you'll have tokenizers 0.8.0 which is incompatible.\u001b[0m\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers, seqeval, tensorboardx, simpletransformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 seqeval-0.0.12 simpletransformers-0.40.2 tensorboardx-2.0 tokenizers-0.8.0 transformers-3.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KEUD3JgUqUrP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593612815623,"user_tz":-60,"elapsed":14877,"user":{"displayName":"nuno amaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoA9ByRphWWfFL33pNwbcmcsHIaScd_KC29jxGqA=s64","userId":"06393260799044096547"}}},"source":["# !pip install transformers==2.10.0"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_YIuiTcY_p9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593612815624,"user_tz":-60,"elapsed":14875,"user":{"displayName":"nuno amaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoA9ByRphWWfFL33pNwbcmcsHIaScd_KC29jxGqA=s64","userId":"06393260799044096547"}}},"source":["# !pip install simpletransformers==0.26.0"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"wh4ol3Y1Zqz3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593612817390,"user_tz":-60,"elapsed":16636,"user":{"displayName":"nuno amaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoA9ByRphWWfFL33pNwbcmcsHIaScd_KC29jxGqA=s64","userId":"06393260799044096547"}}},"source":["from simpletransformers.ner import NERModel\n","from simpletransformers.classification import ClassificationModel"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"AjsoCPI0ckk1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593612818310,"user_tz":-60,"elapsed":17550,"user":{"displayName":"nuno amaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoA9ByRphWWfFL33pNwbcmcsHIaScd_KC29jxGqA=s64","userId":"06393260799044096547"}}},"source":["class PipelineModel:\n","  def __init__(self, train, test, val, ner_type, ner_dir, class_type, class_dir):\n","\n","    print(\"Treating Input DataFrames\")\n","    self.train_df, self.train_codes = self._treat_dataframe(train)\n","    self.test_df, self.test_codes = self._treat_dataframe(test)\n","    self.eval_df, self.eval_codes = self._treat_dataframe(val)\n","\n","    self.id_type, self.class_type = ner_type, class_type\n","    self.id_dir, self.class_dir = ner_dir, class_dir\n","\n","    print(\"Building Span Identification Model\")\n","    self.id_model = self._build_span_model()\n","\n","    # id_model outputs\n","    self.id_results, self.id_preds = None, None\n","    self.id_df = None\n","\n","    # identified spans\n","    self.perfect_spans, self.all_spans = None, None\n","    \n","    print(\"Building Span Classification Model\")\n","    self.class_model = self._build_class_model()\n","\n","    # class_model outputs\n","    self.pclass_results, self.aclass_results = None, None\n","\n","    self.pmodel_outputs, self.amodel_outputs = None, None\n","\n","    self.final_df = None\n","\n","\n","  def process(self):\n","\n","    # Create necessary file in local directory\n","    os.makedirs(os.path.dirname(\"outputs/eval_results.txt\"), exist_ok=True)\n","    with open(\"outputs/eval_results.txt\", \"w\") as f:\n","        f.write(\"\")\n","\n","    # SPAN IDENTIFICATION\n","    print(\"SPAN IDENTIFICATION\")\n","    self.id_result, _, self.id_preds = self.id_model.eval_model(self.test_df, verbose=True)\n","    self.id_df = self._build_id_df()\n","    self.perfect_spans, self.all_spans = self._process_span_id()\n","\n","    print('\\nSpan Identification macro-averaged F-Score: {}\\n'.format(self.f_macro(self.id_df['true'], self.id_df['preds'])))\n","\n","    # SPAN CLASSIFICATION\n","    print(\"SPAN CLASSIFICATION\\n\")\n","    le = LabelEncoder().fit(self.test_codes)\n","    self.perfect_spans['labels'] = le.transform(self.perfect_spans['code'])\n","    self.all_spans['labels'] = le.transform(self.all_spans['code'])\n","\n","    # General Span Classification\n","    print(\"General Span Classification\")\n","    df = self.all_spans.drop(columns=['sid', 'code'])\n","    self.aclass_results, self.amodel_outputs, _ = self.class_model.eval_model(df, acc=metrics.accuracy_score, f1M=self.f_macro)\n","    \n","    print(\"Classification of All Identified Spans:\\n[Categories]\\n{}\\n[Domains]\\n{}\".format(self.aclass_results, self._process_span_class(self.all_spans, self.amodel_outputs)))\n","\n","    pt_results, pt_outputs, _ = self.class_model.eval_model(df[df.country == 'P'], acc=metrics.accuracy_score, f1M=self.f_macro)\n","    print(\"Classification of Portuguese Identified Spans:\\n[Categories]\\n{}\\n[Domains]\\n{}\".format(pt_results, self._process_span_class(self.all_spans[self.all_spans.country == 'P'], pt_outputs)))\n","\n","    br_results, br_outputs, _ = self.class_model.eval_model(df[df.country == 'B'], acc=metrics.accuracy_score, f1M=self.f_macro)\n","    print(\"Classification of Brazilian Identified Spans:\\n[Categories]\\n{}\\n[Domains]\\n{}\".format(br_results, self._process_span_class(self.all_spans[self.all_spans.country == 'B'], br_outputs)))\n","\n","    it_results, it_outputs, _ = self.class_model.eval_model(df[df.country == 'S'], acc=metrics.accuracy_score, f1M=self.f_macro)\n","    print(\"Classification of Italian Identified Spans:\\n[Categories]\\n{}\\n[Domains]\\n{}\".format(it_results, self._process_span_class(self.all_spans[self.all_spans.country == 'S'], it_outputs)))\n","\n","\n","    self.all_spans['preds'] = le.inverse_transform([np.argmax(self.softmax(logits)) for logits in self.amodel_outputs])\n","\n","    self.final_df = self._build_final()\n","    self.final_df.to_csv('pipeline_results.csv', index=False)\n","\n","  def _build_span_model(self):\n","    return NERModel(\n","                 self.id_type,\n","                 self.id_dir,\n","                 labels=['B', 'I', 'O'],\n","                 args={'train_batch_size': 32, \n","                        'eval_batch_size': 32,\n","                        'num_train_epochs': 2,\n","                        'max_seq_length': 200,\n","                        'save_steps': 0,\n","                        'evaluate_during_training': True,\n","                        'evaluate_during_training_steps': int(self.train_df.shape[0] / 32),\n","                        'evaluate_during_training_verbose': True,\n","                        'fp16': False,\n","                        'overwrite_output_dir': True,\n","                        'reprocess_input_data': True,\n","                        'learning_rate': 2e-5,\n","                        'manual_seed':42\n","                        }\n","                 )\n","    \n","  def _build_class_model(self):\n","    return ClassificationModel(\n","                self.class_type,\n","                self.class_dir,\n","                num_labels=46,\n","                args={'train_batch_size': 32, \n","                      'eval_batch_size': 32,\n","                      'num_train_epochs': 4,\n","                      'max_seq_length': 200,\n","                      'save_steps': 0,\n","                      'evaluate_during_training': True,\n","                      'evaluate_during_training_steps': 4422,\n","                      'evaluate_during_training_verbose': True,\n","                      'fp16': False,\n","                      'overwrite_output_dir': True,\n","                      'reprocess_input_data': True,\n","                      'learning_rate': 2e-5,\n","                      'manual_seed':42\n","                      })\n","      \n","  def _build_id_df(self):\n","    test_to_preds = {\"sid\": [], \"words\": [], \"true\": [], \"preds\": []}\n","\n","    for i, sid in self.test_df.groupby(by='sentence_id'):\n","      if sid.shape[0] != len(self.id_preds[i]):\n","        self.id_preds[i] += self.id_preds[i][-1] * (sid.shape[0] - len(self.id_preds[i]))\n","      test_to_preds[\"sid\"].extend(list(sid.sentence_id.values))\n","      test_to_preds[\"words\"].extend(list(sid.words.values))\n","      test_to_preds[\"true\"].extend(list(sid.labels.values))\n","      test_to_preds[\"preds\"].extend(self.id_preds[i])\n","\n","    id_df = pd.DataFrame(test_to_preds)\n","    id_df['codes'] = self.test_codes\n","    id_df['country'] = self.test_df.country\n","\n","    return id_df\n","\n","  \n","  def _build_final(self):\n","\n","    pred_codes = self.all_spans.preds.to_list()\n","\n","    tot_spans = []\n","    span_id = 0\n","\n","    for _, sent_df in self.id_df.groupby(by='sid'):\n","      spans = self.get_spans(sent_df.preds, sent_df.codes)\n","      for start, end in zip(spans, spans[1:]):\n","        df_list = sent_df[start:end].values.tolist()\n","        for sublist in df_list:\n","          sublist.append(pred_codes[span_id])\n","        tot_spans.extend(df_list)\n","        span_id += 1\n","    return pd.DataFrame(tot_spans, columns=['sid', 'words', 'true_id', 'pred_id', 'true_code', 'country', 'pred_code'])\n","\n","\n","  def _process_span_id(self):\n","    perfect = []\n","    all_spans = []\n","    for sentence_id, sent_df in tqdm(self.id_df.groupby(by='sid')):\n","        sent_dict = sent_df.to_dict(orient='list')\n","\n","        true_spans, pred_spans = [self.get_spans(sent_dict[bio], sent_dict[\"codes\"]) for bio in [\"true\", \"preds\"]]\n","\n","        for start, end in zip(pred_spans, pred_spans[1:]):\n","            all_spans.append([sentence_id,\n","                              #self.rebuild_span(sent_dict[\"words\"][start:end]),\n","                              \" \".join(sent_dict[\"words\"][start:end]),\n","                              sent_df[start:end].codes.value_counts().keys()[0]])\n","\n","        for start, end in zip(true_spans, true_spans[1:]):\n","            if (start, end) in zip(pred_spans, pred_spans[1:]) and self.perfect_span(sent_dict, start, end):\n","                perfect.append([sentence_id,\n","                                #self.rebuild_span(sent_dict[\"words\"][start:end]),\n","                                \" \".join(sent_dict[\"words\"][start:end]),\n","                                sent_df[start:end].codes.value_counts().keys()[0]])\n","                \n","    return pd.DataFrame(perfect, columns=['sid', 'text', 'code']), pd.DataFrame(all_spans, columns=['sid', 'text', 'code'])\n","\n","  def _process_span_class(self, test, model_outputs):\n","      label_to_code = dict(zip(test.labels, test.code))\n","\n","      domain_to_label = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7:[], 9:[]}\n","\n","      for label, code in label_to_code.items():\n","        domain = code // 100\n","        domain_to_label[domain].append(label)\n","\n","      true_domains = []\n","\n","      for label in test.labels:\n","        for domain, labels in domain_to_label.items():\n","          if label in labels:\n","            true_domains.append(domain)\n","\n","      preds_bert = [np.argmax(self.softmax(logits)) for logits in model_outputs]\n","\n","      preds_domain_bert = []\n","\n","      for label in preds_bert:\n","        for domain, labels in domain_to_label.items():\n","          if label in labels:\n","            preds_domain_bert.append(domain)\n","\n","      return self.classification_metrics(true_domains, preds_domain_bert)\n","  \n","  def _treat_dataframe(self, df):\n","    df.sentence_id = self.enc(df.sentence_id)\n","    df, df_codes = self.drop_codes(df)\n","    df.words = df.words.apply(lambda x: str(x))\n","    return df, df_codes\n","\n","  def classification_metrics(self, true, preds):\n","    return {\n","            \"acc\": metrics.accuracy_score(true, preds),\n","            \"f_score\": self.f_macro(true, preds),\n","            \"mcc\": metrics.matthews_corrcoef(true, preds)\n","            }\n","\n","  @staticmethod\n","  def enc(sid):\n","    le = LabelEncoder()\n","    sid = [str(id_) for id_ in sid]\n","    return le.fit_transform(sid)\n","\n","  @staticmethod\n","  def drop_codes(df):\n","    return df.drop(['codes'], axis=1), df.codes\n","  \n","  @staticmethod\n","  def get_spans(labels, codes):\n","    limits = []\n","    prev = \"\"\n","    for idx, (lbl, code) in enumerate(zip(labels, codes)):\n","        if lbl == 'B' or (prev != code and lbl == 'O'):\n","            limits.append(idx)\n","        prev = code\n","    limits.append(len(labels))\n","    return limits\n","\n","  @staticmethod\n","  def perfect_span(sent_dict, start, end):\n","    return sent_dict[\"true\"][start:end] == sent_dict[\"preds\"][start:end]\n","\n","  @staticmethod\n","  def rebuild_span(words):\n","    text = \" \".join([str(word) for word in words])\n","    text = re.sub(\"([(]) \", r\"\\1\".rstrip(), text)\n","    return re.sub(\" ([.,:;!?')])\", r\"\\1\".lstrip(), text)\n","\n","  @staticmethod\n","  def f_macro(true, preds):\n","    return metrics.f1_score(true, preds, average='macro')\n","\n","  @staticmethod\n","  def softmax(logits):\n","    return np.exp(logits) / np.sum(np.exp(logits), axis=0)\n","\n","  @staticmethod\n","  def domain_label(label_to_code):\n","    domain_to_label = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7:[], 9:[]}\n","    for label, code in label_to_code.items():\n","      domain = code // 100\n","      domain_to_label[domain].append(label)\n","    return domain_to_label"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"WmMZvHV7D18Q","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593612824403,"user_tz":-60,"elapsed":23638,"user":{"displayName":"nuno amaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoA9ByRphWWfFL33pNwbcmcsHIaScd_KC29jxGqA=s64","userId":"06393260799044096547"}}},"source":["test = pd.read_csv('drive/My Drive/data-ner/test_ner.csv')\n","train = pd.read_csv('drive/My Drive/data-ner/train_ner.csv')\n","val = pd.read_csv('drive/My Drive/data-ner/val_ner.csv')"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfQ2L8Oh_e87","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1593612826277,"user_tz":-60,"elapsed":25493,"user":{"displayName":"nuno amaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoA9ByRphWWfFL33pNwbcmcsHIaScd_KC29jxGqA=s64","userId":"06393260799044096547"}},"outputId":"9c7d8fa6-97e0-445e-a5f3-b1b2373b67d3"},"source":["from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","t = pd.read_csv('drive/My Drive/data-class/test.csv', names=['text', 'Code'], header=0)\n","t[\"labels\"] = le.fit_transform(t.Code)\n","t = t[[\"text\", \"labels\"]]\n","t.head(5)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>La ministra colombiana de Exteriores considera...</td>\n","      <td>37</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>DEPORTES</td>\n","      <td>31</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>É enorme a importância dessas atividades no de...</td>\n","      <td>31</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Com o fim da guerra fria, iniciou-se um novo p...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Revelaram-se com inapagável nitidez novas teia...</td>\n","      <td>13</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  labels\n","0  La ministra colombiana de Exteriores considera...      37\n","1                                           DEPORTES      31\n","2  É enorme a importância dessas atividades no de...      31\n","3  Com o fim da guerra fria, iniciou-se um novo p...       5\n","4  Revelaram-se com inapagável nitidez novas teia...      13"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"nqZRObXYZcYH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593612826278,"user_tz":-60,"elapsed":25490,"user":{"displayName":"nuno amaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoA9ByRphWWfFL33pNwbcmcsHIaScd_KC29jxGqA=s64","userId":"06393260799044096547"}}},"source":["# test.shape"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"cFq5FlzvZaZ_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593612826279,"user_tz":-60,"elapsed":25484,"user":{"displayName":"nuno amaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoA9ByRphWWfFL33pNwbcmcsHIaScd_KC29jxGqA=s64","userId":"06393260799044096547"}}},"source":["# test = pd.concat([test, val]).reset_index(drop=True)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUM2jLIJa2bO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593612826279,"user_tz":-60,"elapsed":25478,"user":{"displayName":"nuno amaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoA9ByRphWWfFL33pNwbcmcsHIaScd_KC29jxGqA=s64","userId":"06393260799044096547"}}},"source":["# test.shape"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"hGx_xGliFdw2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1593612868023,"user_tz":-60,"elapsed":67206,"user":{"displayName":"nuno amaro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoA9ByRphWWfFL33pNwbcmcsHIaScd_KC29jxGqA=s64","userId":"06393260799044096547"}},"outputId":"49dc78a2-bbd4-4e67-c11c-75e1acaca17a"},"source":["model = PipelineModel(train, test, val, 'bert', 'drive/My Drive/bert-ner/', 'bert', 'drive/My Drive/bert-class/')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Treating Input DataFrames\n","Building Span Identification Model\n","Building Span Classification Model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"htoSewS_JVPs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"346ae7c2-8572-43a8-d10c-c05a68d8ab33"},"source":["model.process()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["SPAN IDENTIFICATION\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yFu7xebjna45","colab_type":"code","colab":{}},"source":["# pred_codes = model.all_spans.preds.to_list()\n","# len(pred_codes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4gClg_99l78V","colab_type":"code","colab":{}},"source":["# tot_spans = []\n","# span_id = 0\n","# for _, sent_df in model.id_df.groupby(by='sid'):\n","#   spans = model.get_spans(sent_df.preds, sent_df.codes)\n","#   for start, end in zip(spans, spans[1:]):\n","#     df_list = sent_df[start:end].values.tolist()\n","#     for sublist in df_list:\n","#       sublist.append(pred_codes[span_id])\n","#     tot_spans.extend(df_list)\n","#     span_id += 1\n","# len(tot_spans)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ZVhZZoSvoag","colab_type":"code","colab":{}},"source":["# df = pd.DataFrame(tot_spans, columns=['sid', 'words', 'true_id', 'pred_id', 'true_code', 'country', 'pred_code'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-6NsCedJ5Yg","colab_type":"code","colab":{}},"source":["# df.to_csv('pipeline.csv', index=False)"],"execution_count":null,"outputs":[]}]}